{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"A4.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 (10 Points in total, will be 10% of your grade)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to complete assignments\n",
    "\n",
    "There are both code and math/text components to these assignments.\n",
    "\n",
    "### Math/text responses\n",
    "\n",
    "Whenever you see:\n",
    "\n",
    "```markdown\n",
    "_Type your answer here, replacing this text._ \n",
    "```\n",
    "\n",
    "You should editing this Markdown cell and insert your answers. \n",
    "\n",
    "If the question is asking for math use LaTeX. We understand not everyone is fluent in LaTeX but we think this is a good skill to learn. [Here is a fantastic tutorial from CalTech about using $\\LaTeX$ in Jupyter Notebook.](http://chebe163.caltech.edu/2018w/handouts/intro_to_latex.html). You could also find various $\\LaTeX$ tutorials and cheat sheets online. \n",
    "\n",
    "Some things to keep in mind:  Jupyter's Markdown-LaTeX will display things that don't work when you turn in the assignment. Here are some generic ideas that we find prevent trouble down the road. \n",
    "1. Test your output to see if it works well when you submit.  So execute the otter grader export and check the PDF to make sure it looks right!!! \n",
    "1. Avoid ```$x=1$ ``` use ```$$x=1$$``` instead\n",
    "1.  No trailing space after the first dollar sign and before the second dollar sign. i.e. don't do `$ $ x = 5 $ $` use `$$x=5$$` instead\n",
    "1. Markdown lists in the same cell after a math environment don't seem to work, and we don't know why\n",
    "1. Avoid ```\\align``` use ```\\aligned``` instead\n",
    "\n",
    "An example:\n",
    "\n",
    "\n",
    "#### Question: Given a triangle with sides $x,y,z$ where $x=3$, $y=4$ calculate $z$\n",
    "\n",
    "Your answer in the markdown should be\n",
    "\n",
    "```\n",
    "$$z = \\sqrt{x^2 + y^2} = \\sqrt{3^2+4^2} = \\sqrt{25} = 5$$\n",
    "```\n",
    "which will render as \n",
    "\n",
    "$$z = \\sqrt{x^2 + y^2} = \\sqrt{3^2+4^2} = \\sqrt{25} = 5$$\n",
    "\n",
    "\n",
    "These responses will mostly be manually graded. There is leeway for individual style in notation, so don't worry too much as long as you don't write anything that is false. \n",
    "\n",
    "### Code responses\n",
    "\n",
    "Whenever you see:\n",
    "\n",
    "```python\n",
    "answers = ...\n",
    "```\n",
    "\n",
    "You need to replace this section with some code that answers the questions and meets the specified criteria. Make sure you remove the 'raise' line when you do this (or your notebook will raise an error, regardless of any other code, and thus fail the grading tests).\n",
    "\n",
    "You should write the answer to the questions in those cells (the ones with `...`), but you can also add extra cells to explore / investigate things if you need / want to. \n",
    "\n",
    "Any cell with `grader.check(\"Question_id\")` statements in it is a test cell. You should not try to change or delete these cells. Note that there might be more than one assert that tests a particular question. Once you run the cell, you will see the public tests we provided for you.\n",
    "\n",
    "If a test does fail, reading the error that is printed out should let you know which test failed, which may be useful for fixing it.\n",
    "\n",
    "Note that some cells, including the test cells, may be read only, which means they won't let you edit them. If you cannot edit a cell - that is normal, and you shouldn't need to edit that cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 1: Multiple Choice (1 Points)\n",
    "\n",
    "For each questions, assign your answer to the list with the correct choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Question 1.1:** Which of the following is a true property of the Kullback-Leibler divergence?\n",
    "\n",
    "A): KL divergence is always negative.\n",
    "\n",
    "B): KL divergence can be used to measure the distance between two distributions in both directions interchangeably, i.e. $KL(P||Q) = KL (Q||P)$ is true for every distribution $Q$ and $P$\n",
    "\n",
    "C): KL divergence is zero if and only if the two distributions being compared are identical.\n",
    "\n",
    "D): KL divergence satisfies the triangle inequality.\n",
    "\n",
    "**Question 1.2:** When using the KL divergence to compare two probability distributions $P$ and $Q$, a small KL divergence value indicates that:\n",
    "\n",
    "A): The distributions $P$ and $Q$ are very similar.\n",
    "\n",
    "B): The distribution $Q$ is a good approximation of distribution $P$.\n",
    "\n",
    "C): We would not be very surprised when observing data generated by distribution \n",
    "$P$ when our model of the data was $Q$.\n",
    "\n",
    "D): All of the above\n",
    "\n",
    "**Question 1.3:** The perplexity parameter in t-SNE:\n",
    "\n",
    "A): Is independent of the dataset size and only affects the computational complexity of the algorithm.\n",
    "\n",
    "B): Directly controls the learning rate of the gradient descent optimization in t-SNE.\n",
    "\n",
    "C): Can be thought of as a measure of the effective number of neighbors for each point and influences the balance between local and global aspects of the data.\n",
    "\n",
    "D): Sets the number of dimensions to which the data will be reduced.\n",
    "\n",
    "**Question 1.4:** Which of the following best describes the principle of operation for both t-SNE and UMAP?\n",
    "\n",
    "A): They both linearly project data into lower-dimensional spaces while preserving global data structure.\n",
    "\n",
    "B): They both aim to preserve the local neighborhood structure of the data in a lower-dimensional space.\n",
    "\n",
    "C): They both primarily focus on preserving the pairwise distances between all points in the dataset.\n",
    "\n",
    "D): They both use a global optimization technique to ensure that similar data points in the high-dimensional space are mapped far apart in the lower-dimensional representation.\n",
    "\n",
    "**Question 1.5:** The absolute positions of clusters within the embedding space have a direct and interpretable meaning.\n",
    "\n",
    "A): True\n",
    "\n",
    "B): False\n",
    "\n",
    "**Question 1.6** Compared to t-SNE, UMAP:\n",
    "\n",
    "A) Is generally faster and more scalable, particularly on larger datasets.\n",
    "\n",
    "B) Can only visualize datasets in two or three dimensions, while t-SNE can project data into any number of dimensions.\n",
    "\n",
    "C) Lacks the ability to preserve local neighborhood structures of the data.\n",
    "\n",
    "D) Requires more hyperparameters to be tuned for optimal performance.\n",
    "\n",
    "_Points:_ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "multiple_choices = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"11_multiple_choice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: KL-Divergence (2 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KL-Divergence is a measure of how one probability distribution $p(x)$ is different from another distribution $q(x)$.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "D_{KL}(p(x)||q(x)) &= \\mathbb{E}_p(x)\\left[\\log\\frac{p(x)}{q(x)}\\right] \\\\\n",
    "&= \\int_{-\\infty}^{\\infty} p(x)\\log \\frac{p(x)}{q(x)} dx & \\text{for continuous distribution $p$ and $q$} \\\\\n",
    "&= \\sum p(x)\\log \\frac{p(x)}{q(x)} & \\text{for discrete distribution $p$ and $q$} \\\\\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 2.1: Prove that KL Divergence is Non-negative\n",
    "\n",
    "Prove that \n",
    "$$D_{KL}(p(x)||q(x))\\geq 0$$\n",
    "\n",
    "Hint:\n",
    "- Apply Jensen's inequality, which says that $\\mathbb{E}[f(z)]\\geq f(\\mathbb{E}[z])$ for any convex function $f:\\mathbb{R}\\to\\mathbb{R}$ and for any random variable $z$.\n",
    "- In this case, you should use $f(z) = -\\log(z)$\n",
    "- Note that $\\log\\frac{a}{b} = \\log(a)-\\log(b) = -(\\log(b)-\\log(a)) = -\\log\\frac{b}{a}$\n",
    "- Think about the case where the divergence would be lowest... when the two distributions are the same!\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Question 3: Explore PCA v.s. TSNE v.s. UMAP (4 Points)\n",
    "\n",
    "Let's have you use some interesting data to explore the differences and similarities between dimensionality reduction techniques.\n",
    "\n",
    "First off we will get UMAP installed for you. \n",
    "\n",
    "Then we will download some data with stock prices for 56 large global companies between 2003 and 2007 (a time when the market overall was growing rapidly and before the 2008 crash). These companies come from several different kinds of industries... tech, finance, manufacturing, pharmaceutical, etc.  If you aren't already familiar with these companies and their industries, you really should investigate a little or you won't be able to talk about the results of your analysis properly.  \n",
    "\n",
    "Lastly we will transform the data in ways that make it easier to find patterns... please make sure you understand how and why these transformations are helpful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first off we will need umap\n",
    "# this is maybe not necessary if you have the newest sklearn\n",
    "# but assuming you have our environment for this class it is needed\n",
    "%pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "sns.set(style='white', context='notebook', rc={'figure.figsize':(12,8)})\n",
    "\n",
    "# plotting helper function\n",
    "def plot_components(components,x='pc1',y='pc2'):\n",
    "    '''\n",
    "    2d scatter plot of dimensionality reductions with labelling\n",
    "\n",
    "    inputs:\n",
    "        components (pd.DataFrame) shape is (samples x components), index is sample names\n",
    "        x (str) name of column in components to plot as x axis\n",
    "        y (str) name of column in components to plot as y axis\n",
    "\n",
    "    typically you might generate components like this\n",
    "        pcs = PCA(n_components=2).fit_transform(daily_change)\n",
    "        components = pd.DataFrame(pcs,columns=['pc1','pc2'])\n",
    "        components.index = daily_change.index\n",
    "    '''\n",
    "    sns.scatterplot(data=components, x=x, y=y)\n",
    "    sns.despine()\n",
    "    ax = plt.gca()\n",
    "    for company in components.index:\n",
    "      ax.annotate(company, xy=components.loc[company])\n",
    "\n",
    "symbol_dict = {\n",
    "    \"TOT\": \"Total\",\n",
    "    \"XOM\": \"Exxon\",\n",
    "    \"CVX\": \"Chevron\",\n",
    "    \"COP\": \"ConocoPhillips\",\n",
    "    \"VLO\": \"Valero Energy\",\n",
    "    \"MSFT\": \"Microsoft\",\n",
    "    \"IBM\": \"IBM\",\n",
    "    \"TWX\": \"Time Warner\",\n",
    "    \"CMCSA\": \"Comcast\",\n",
    "    \"CVC\": \"Cablevision\",\n",
    "    \"YHOO\": \"Yahoo\",\n",
    "    \"DELL\": \"Dell\",\n",
    "    \"HPQ\": \"HP\",\n",
    "    \"AMZN\": \"Amazon\",\n",
    "    \"TM\": \"Toyota\",\n",
    "    \"CAJ\": \"Canon\",\n",
    "    \"SNE\": \"Sony\",\n",
    "    \"F\": \"Ford\",\n",
    "    \"HMC\": \"Honda\",\n",
    "    \"NAV\": \"Navistar\",\n",
    "    \"NOC\": \"Northrop Grumman\",\n",
    "    \"BA\": \"Boeing\",\n",
    "    \"KO\": \"Coca-Cola\",\n",
    "    \"MMM\": \"3M\",\n",
    "    \"MCD\": \"McDonald's\",\n",
    "    \"PEP\": \"Pepsi\",\n",
    "    \"K\": \"Kellogg\",\n",
    "    \"UN\": \"Unilever\",\n",
    "    \"MAR\": \"Marriott\",\n",
    "    \"PG\": \"Procter Gamble\",\n",
    "    \"CL\": \"Colgate-Palmolive\",\n",
    "    \"GE\": \"General Electric\",\n",
    "    \"WFC\": \"Wells Fargo\",\n",
    "    \"JPM\": \"JPMorgan Chase\",\n",
    "    \"AIG\": \"AIG\",\n",
    "    \"AXP\": \"American Express\",\n",
    "    \"BAC\": \"Bank of America\",\n",
    "    \"GS\": \"Goldman Sachs\",\n",
    "    \"AAPL\": \"Apple\",\n",
    "    \"SAP\": \"SAP\",\n",
    "    \"CSCO\": \"Cisco\",\n",
    "    \"TXN\": \"Texas Instruments\",\n",
    "    \"XRX\": \"Xerox\",\n",
    "    \"WMT\": \"Wal-Mart\",\n",
    "    \"HD\": \"Home Depot\",\n",
    "    \"GSK\": \"GlaxoSmithKline\",\n",
    "    \"PFE\": \"Pfizer\",\n",
    "    \"SNY\": \"Sanofi-Aventis\",\n",
    "    \"NVS\": \"Novartis\",\n",
    "    \"KMB\": \"Kimberly-Clark\",\n",
    "    \"R\": \"Ryder\",\n",
    "    \"GD\": \"General Dynamics\",\n",
    "    \"RTN\": \"Raytheon\",\n",
    "    \"CVS\": \"CVS\",\n",
    "    \"CAT\": \"Caterpillar\",\n",
    "    \"DD\": \"DuPont\",\n",
    "}\n",
    "\n",
    "\n",
    "symbols, names = np.array(sorted(symbol_dict.items())).T\n",
    "\n",
    "quotes = []\n",
    "\n",
    "for symbol in symbols:\n",
    "    print(\"Fetching quote history for %r\" % symbol, file=sys.stderr)\n",
    "    url = (\n",
    "        \"https://raw.githubusercontent.com/scikit-learn/examples-data/\"\n",
    "        \"master/financial-data/{}.csv\"\n",
    "    )\n",
    "    quotes.append(pd.read_csv(url.format(symbol)))\n",
    "\n",
    "close_prices = np.vstack([q[\"close\"] for q in quotes])\n",
    "open_prices = np.vstack([q[\"open\"] for q in quotes])\n",
    "\n",
    "# Stock prices are non-stationary time series data... \n",
    "# if you don't know what that means read this https://otexts.com/fpp2/stationarity.html\n",
    "# taking the daily difference in stock price helps with non-stationarity by removing big trends\n",
    "# looking at that daily difference as a ratio of change\n",
    "# normalizes somewhat across stocks with very different prices\n",
    "# and may help us discover groups of stocks that \n",
    "# move up and down in lock step but at different raw prices\n",
    "daily_diff = close_prices - open_prices\n",
    "daily_change = pd.DataFrame(daily_diff/open_prices, index=names, columns=quotes[0]['date'])\n",
    "daily_change.index.name = 'company'\n",
    "daily_change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 3.1: PCA plot\n",
    "\n",
    "Load the necessary things from scikit-learn, do the data transform, use the helper function to plot the results\n",
    "\n",
    "_Points:_ 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = ...\n",
    "\n",
    "# call fit_transform on pca\n",
    "pcs = ...\n",
    "# transform it into data frame\n",
    "pcs = pd.DataFrame(pcs,columns=['pc1','pc2'])\n",
    "pcs.index = daily_change.index\n",
    "# plot the components\n",
    "plot_components(pcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"31_plot_pca\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 3.2: t-SNE plot\n",
    "\n",
    "Load the necessary things from scikit-learn, do the data transform, use the helper function to plot the results\n",
    "\n",
    "Use a `random_state=99` and explore the following values for `perplexity`: 5, 20, and 40 (and any others you feel like)\n",
    "\n",
    "_Points:_ 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "tsne_perp_5 = ...\n",
    "tcs = ...\n",
    "# construct the data frame\n",
    "tcs = pd.DataFrame(tcs,columns=['c1','c2'])\n",
    "tcs.index = daily_change.index\n",
    "\n",
    "plot_components(tcs,'c1','c2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tsne_perp_20 = ...\n",
    "tcs = ...\n",
    "tcs = pd.DataFrame(tcs,columns=['c1','c2'])\n",
    "tcs.index = daily_change.index\n",
    "\n",
    "plot_components(tcs,'c1','c2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tsne_perp_40 = ...\n",
    "tcs = ...\n",
    "tcs = pd.DataFrame(tcs,columns=['c1','c2'])\n",
    "tcs.index = daily_change.index\n",
    "\n",
    "plot_components(tcs,'c1','c2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"32_plot_tsne\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 3.3: UMAP plot\n",
    "\n",
    "Load the necessary things from umap, do the data transform, use the helper function to plot the results\n",
    "\n",
    "Use a `random_state=99` and `min_dist=0.1` and explore the following values for `n_neighbors`: 5, 20, and 40 (and any others you feel like)\n",
    "\n",
    "You'll probably want to take a look at the UMAP docs: https://umap-learn.readthedocs.io/en/latest/parameters.html\n",
    "\n",
    "\n",
    "\n",
    "_Points:_ 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from umap import UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "umap_5_neigh = ...\n",
    "ucs = ...\n",
    "\n",
    "ucs = pd.DataFrame(ucs,columns=['c1','c2'])\n",
    "ucs.index = daily_change.index\n",
    "plot_components(ucs,'c1','c2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "umap_20_neigh = ...\n",
    "ucs = ...\n",
    "\n",
    "ucs = pd.DataFrame(ucs,columns=['c1','c2'])\n",
    "ucs.index = daily_change.index\n",
    "plot_components(ucs,'c1','c2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "umap_40_neigh = ...\n",
    "ucs = ...\n",
    "\n",
    "ucs = pd.DataFrame(ucs,columns=['c1','c2'])\n",
    "ucs.index = daily_change.index\n",
    "plot_components(ucs,'c1','c2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"33_plot_umap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 3.4: Compare and discuss\n",
    "\n",
    "Let's discuss what you saw with the various plots. \n",
    "\n",
    "Particularly we are looking to see how companies appear to be similar to each other in the reduced dimensions.  If two companies are near each other in low-d assumedly their stocks move up and down together. This might happen because they are in the same industry and there are good weeks and bad weeks for oil stocks that are different than good/bad weeks for tech stocks.  But industry isn't everything; clearly inside an industry there could be companies doing well while others do poorly. So looking for clusters of industry is simply something I assume might be there, but doesn't have to be. Think of this as the default \"pattern\" I'm asking you about in the questions below.\n",
    "\n",
    "**NOTE:** I'm not a broker or an economist, so there are probably other reasons for companies clustering together or separating from each other. If you have particular expertise or are gung-ho to do some research about these companies in this time period I'm curious to see what other things you come up with. So hit me with your best explanations for why \"patterns\" exist in the low-d data! Think of this as a mini project... this is the kind of subject matter expertise that you will need to bring to your projects :)\n",
    "\n",
    "So in the Markdown box below please address the following questions in a short essay format. _Your response should probably be 1 (and no more than 2!) paragraphs per bullet point below._ \n",
    "- Characterize what the PCA plot looks like, say what patterns you can see\n",
    "- Characterize differences among t-SNE plots, say what patterns you can see. Talk about how these patterns appear/disappear/modify with the different parameters. Relate these pattern changes to what you know about the nature of high and low perplexity\n",
    "- Characterize differences among UMAP plots, say what patterns you can see. Talk about how these patterns appear/disappear/modify with the different parameters. Relate these pattern changes to what you know about the nature of high and low n_neighbors\n",
    "- Why didn't I ask you to investigate even larger values for perplexity or n_neighbors, like e.g. 100? What happens when you try that with the algorithm?\n",
    "- What differences do you see in the plots between the 3 algorithms? Do you feel like there's a strong reason to prefer one of these algorithms? Reasons might include results, computational complexity, hyper-parameter tuning, and anything else you feel is relevant\n",
    "- Given what you've seen here, what conclusions do you feel comfortable drawing about these stocks and why? What else do you suspect but would feel like you can't prove given this analysis?\n",
    "\n",
    "\n",
    "_Points:_ 3.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Question 4: Use UMAP as the Basis for a Supervised Classifier (4 Points)\n",
    "\n",
    "Heres an example of how to use UMAP as the input to a supervised learning classifier... You'll have to look some stuff up about UMAP https://umap-learn.readthedocs.io/en/latest/transform.html and k-Nearest-Neighbors https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-classification\n",
    "\n",
    "In this example, we will:\n",
    "\n",
    "- get the full 70k sample x 784 dimension MNIST dataset\n",
    "- create a pipeline to \n",
    "    - make a dimension reduction using UMAP down to a 2 dimension embedding\n",
    "    - use a k-Nearest Neighbors classifier in that 2d embedding to classify digits\n",
    "- visualize the results of the predictions in the embedding \n",
    "\n",
    "Some interesting notes on this process:\n",
    "\n",
    "- UMAP and kNN are similar in that large numbers of neighbors produces smoother results.. more global relationships for UMAP and more smooth decision boundaries for kNN. Both of these are _VERY_ dependent on hyperparameters!\n",
    "- We are setting you up with a pipeline version of this process in order to facilitate you doing some hyperparameter selection in the next question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "from umap import UMAP\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "# fetch dataset\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "mnist.target = mnist.target.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50/50 train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    mnist.data, mnist.target, test_size=0.5\n",
    ")\n",
    "\n",
    "# arbitrary parameters, feel free to play with this\n",
    "trans = UMAP(n_neighbors=5, min_dist=0.1)\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('dim_reduce', trans),\n",
    "    ('classifier', knn)\n",
    "])\n",
    "\n",
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is how you can extract the embedding from the training set data\n",
    "# .steps is a list of the steps of the pipeline\n",
    "# [0] is the first one (dimred)\n",
    "# which yields the dimreduce tuple from the Pipeline command, \n",
    "# [1] is the actual umap class while [0] is just the name\n",
    "# finally inside the umap instance .embedding_ is the transformed training data\n",
    "embedding_train = pipe.steps[0][1].embedding_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "scatter = plt.scatter(embedding_train[:, 0], embedding_train[:, 1], c=y_train.astype(int), s=0.1, cmap='Spectral')\n",
    "handles, _ = scatter.legend_elements(prop=\"colors\", alpha=0.6) # use my own labels\n",
    "legend1 = plt.legend(handles, np.arange(0, 10))\n",
    "plt.title('Test set, colored by prediction');\n",
    "plt.title('Training set, colored by true label')\n",
    "plt.xlabel(\"Embedding Dimension 1\")\n",
    "plt.ylabel(\"Embedding Dimension 2\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 4.1: Make a prediction on the test set\n",
    "\n",
    "_Points:_ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# how can we now get predictions from the fitted pipeline? basic scikit-learn usage!\n",
    "prediction = ...\n",
    "print(classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"41_UMAP_Basis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get the test set embedding we need must explictly call dimred.transform() on the test data\n",
    "# to call dimred.transform() we use the pipe.steps trick again\n",
    "\n",
    "# note this .transform() is an approximation!!!\n",
    "# The vanilla UMAP cannot transform test data its never seen, because the embedding is produced\n",
    "# during a gradient descent process. However there is various black magic being done here\n",
    "# where another supervised algorithm is trained to try to go from original training data to \n",
    "# the low d embedding, and that supervised algorithm is used to transform the test data\n",
    "# if you're interested here's discussions of HOW this black magic happens https://github.com/lmcinnes/umap/issues/40\n",
    "\n",
    "embedding_test = pipe.steps[0][1].transform(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at all that salt and pepper in scatter dots in between the \"clusters\"\n",
    "# these are \"outlier\" digits that don't really look like anything in the training data\n",
    "# so they really don't fall where other digits of that label did\n",
    "plt.figure(figsize=(10, 10))\n",
    "scatter = plt.scatter(embedding_test[:, 0], embedding_test[:, 1], c=prediction.astype(int), s=0.1, cmap='Spectral');\n",
    "handles, _ = scatter.legend_elements(prop=\"colors\", alpha=0.6) # use my own labels\n",
    "legend1 = plt.legend(handles, np.arange(0, 10))\n",
    "plt.title('Test set, colored by prediction');\n",
    "plt.xlabel(\"Embedding Dimension 1\")\n",
    "plt.ylabel(\"Embedding Dimension 2\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 4.2: Model Selections\n",
    "\n",
    "OK its your turn!!\n",
    "\n",
    "As you can see a pipeline sets you up to do proper model selection on hyperparameters.\n",
    "\n",
    "We had several notebooks that covered gridsearch (8, 12, & 15); the last two addressed it specifically with pipelines.  But this will be a little different as we are using a supervised learning algorithm on top of the dimensionality reduction, and the metrics you will probably want are things like accuracy rather than BIC.  Heres' a sklearn demo that uses supervised learning on top of dimensionality reduction that's relevant https://scikit-learn.org/stable/auto_examples/compose/plot_compare_reduction.html#sphx-glr-auto-examples-compose-plot-compare-reduction-py\n",
    "\n",
    "Anyway... go for it! Pick a semi reasonable set of parameters for UMAP and kNN and see how good you can make the test set error!\n",
    "\n",
    "**Notes:** `GridSearchCV` is very computationally intensive (think about why). Therefore, you need to choose your hyperparamter grid wisely. We advise that you do not exceed a total of 16 possible models for your grid search, but feel free to have more if you have a fancy multi-core CPU.\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We will continue to use the NMIST dataset for this question\n",
    "pipe = Pipeline([\n",
    "    # fill in your pipeline\n",
    "    (...), # UMAP dimensionality reduction\n",
    "    (...), # kNN classifier\n",
    "])\n",
    "\n",
    "...\n",
    "\n",
    "param_grid = {\n",
    "    \"...\": ..., # UMAP hyper-parameters\n",
    "    \"...\": ..., # kNN hyper-parameters\n",
    "}\n",
    "\n",
    "...\n",
    "\n",
    "# create a GridSearchCV Object with the pipe and the param_grid, you can set n_jobs=-1 to enable parallelism\n",
    "# Do a two fold validation by setting the cv=2, and verbose=5\n",
    "grid = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fit the grid with the training data\n",
    "# this will take considerable amount of time.\n",
    "# feel free to train on the whole dataset, but for the time being,\n",
    "# you can train on the first 5k samples\n",
    "\n",
    "fitted = grid.fit(X_train[:5000], y_train[:5000]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"42_UMAP_model_selection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 4.3: Model Performance Evaluations\n",
    "\n",
    "The following code visualize the performance of each individual model in the `GridSearchCV`. Comment on which model performs the best, with which set of hyperparameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame(grid.cv_results_['params'] )\n",
    "results['score'] = grid.cv_results_['mean_test_score']\n",
    "# heres a tibble of your grid search results\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# this makes a 2d heatmap comparing JUST TWO of the parameters you are interested in \n",
    "\n",
    "# YOU MUST INTERVENE HERE\n",
    "# the first two args to .pivot (index, columns) \n",
    "# must be the names of the parameters you want to score\n",
    "# right now this assumes you are comparing the first two params\n",
    "# modify it according to what you need to visualize!!!\n",
    "# if you're doing 3 params, you may need to make 2 heatmaps, one for each pair of params\n",
    "\n",
    "grid_params = results.columns\n",
    "heatvals = results.pivot(index=grid_params[0],\n",
    "                           columns=grid_params[1],\n",
    "                           values='score')\n",
    "plot = sns.heatmap(heatvals, annot=True, fmt='5.4f', robust=True, cmap=\"crest\");\n",
    "plot.figure.savefig(\"./data/43plot.png\")\n",
    "heatvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# NB some people have a problem where you only see the first row\n",
    "# of the numeric annotations in the heatmap above\n",
    "# if that affects you no big deal.  thats why I made it print the results grid too\n",
    "# if you want, you can try to uncomment the following, run it, and restart your kernel\n",
    "# %pip install seaborn --upgrade "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "![](./data/43plot.png)\n",
    "\n",
    "_Points:_ 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## The End of A4\n",
    "\n",
    "Have a look back over your answers, and also make sure to `Restart & Run All` from the kernel menu to double check that everything is working properly. This restarts everything and runs your code from top to bottom.\n",
    "\n",
    "Once you're happy with your work, click the disk icon to save, and submit the zip file onto gradescope. **You MUST submit all the required component to receive credit.**\n",
    "\n",
    "Note that you can submit at any time, but **we grade your most recent submission**. This means that **if you submit an updated notebook after the submission deadline, it will be marked as late**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note:_ If you encountered `LatexFailed` message during exporting, or has the `AttributeError: module 'nbconvert' has no attribute 'pdf'` error, it indicates that your $\\LaTeX$ code is not correct. Try to the LaTeX syntax error by scrolling up to see the LaTeX error message. If you need any additional helps, please make a private post on campuswire and we are happy to help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit.\n",
    "\n",
    "Please make sure to see the output of the gradescope autograder. You are responsible for waiting and ensuring that the autograder is executing normally for your submission. Please create a campuswire post if you see errors in autograder execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.export(force_save=True, run_tests=True, files=['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "11_multiple_choice": {
     "name": "11_multiple_choice",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isinstance(multiple_choices, list)\n>>> assert all([choice in ['A', 'B', 'C', 'D'] for choice in multiple_choices])\n",
         "hidden": false,
         "locked": false,
         "points": 0
        },
        {
         "code": ">>> assert multiple_choices[0] == 'C'\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert multiple_choices[1] == 'D'\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert multiple_choices[2] == 'C'\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert multiple_choices[3] == 'B'\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert multiple_choices[4] == 'B'\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert multiple_choices[5] == 'A'\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "31_plot_pca": {
     "name": "31_plot_pca",
     "points": 0.25,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isinstance(pca, PCA)\n>>> assert isinstance(pcs, pd.DataFrame)\n",
         "hidden": false,
         "locked": false,
         "points": 0
        },
        {
         "code": ">>> assert np.allclose((2, 1258), pca.components_.shape)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "32_plot_tsne": {
     "name": "32_plot_tsne",
     "points": 0.25,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> params = tsne_perp_5.get_params()\n>>> assert np.isclose(params['perplexity'], 5)\n>>> assert np.isclose(params['random_state'], 99)\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> params = tsne_perp_20.get_params()\n>>> assert np.isclose(params['perplexity'], 20)\n>>> assert np.isclose(params['random_state'], 99)\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> params = tsne_perp_40.get_params()\n>>> assert np.isclose(params['perplexity'], 40)\n>>> assert np.isclose(params['random_state'], 99)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "33_plot_umap": {
     "name": "33_plot_umap",
     "points": 0.25,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> params = umap_5_neigh.get_params()\n>>> assert np.isclose(params['n_neighbors'], 5)\n>>> assert np.isclose(params['random_state'], 99)\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> params = umap_20_neigh.get_params()\n>>> assert np.isclose(params['n_neighbors'], 20)\n>>> assert np.isclose(params['random_state'], 99)\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> params = umap_40_neigh.get_params()\n>>> assert np.isclose(params['n_neighbors'], 40)\n>>> assert np.isclose(params['random_state'], 99)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "41_UMAP_Basis": {
     "name": "41_UMAP_Basis",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isinstance(prediction, np.ndarray)\n>>> assert np.isclose(prediction.shape, y_test.shape)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "42_UMAP_model_selection": {
     "name": "42_UMAP_model_selection",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isinstance(pipe, Pipeline)\n>>> assert isinstance(grid, GridSearchCV)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
