{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"A2.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 (10 Points in total, will be 10% of your grade)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to complete assignments\n",
    "\n",
    "There are both code and math/text components to these assignments.\n",
    "\n",
    "### Math/text responses\n",
    "\n",
    "Whenever you see:\n",
    "\n",
    "```markdown\n",
    "_Type your answer here, replacing this text._ \n",
    "```\n",
    "\n",
    "You should editing this Markdown cell and insert your answers. \n",
    "\n",
    "If the question is asking for math use LaTeX. We understand not everyone is fluent in LaTeX but we think this is a good skill to learn. [Here is a fantastic tutorial from CalTech about using $\\LaTeX$ in Jupyter Notebook.](http://chebe163.caltech.edu/2018w/handouts/intro_to_latex.html). You could also find various $\\LaTeX$ tutorials and cheat sheets online. \n",
    "\n",
    "Some things to keep in mind:  Jupyter's Markdown-LaTeX will display things that don't work when you turn in the assignment. Here are some generic ideas that we find prevent trouble down the road. \n",
    "1. Test your output to see if it works well when you submit.  So execute the otter grader export and check the PDF to make sure it looks right!!! \n",
    "1. Avoid ```$x=1$ ``` use ```$$x=1$$``` instead\n",
    "1.  No trailing space after the first dollar sign and before the second dollar sign. i.e. don't do `$ $ x = 5 $ $` use `$$x=5$$` instead\n",
    "1. Markdown lists in the same cell after a math environment don't seem to work, and we don't know why\n",
    "1. Avoid ```\\align``` use ```\\aligned``` instead\n",
    "\n",
    "An example:\n",
    "\n",
    "\n",
    "#### Question: Given a triangle with sides $x,y,z$ where $x=3$, $y=4$ calculate $z$\n",
    "\n",
    "Your answer in the markdown should be\n",
    "\n",
    "```\n",
    "$$z = \\sqrt{x^2 + y^2} = \\sqrt{3^2+4^2} = \\sqrt{25} = 5$$\n",
    "```\n",
    "which will render as \n",
    "\n",
    "$$z = \\sqrt{x^2 + y^2} = \\sqrt{3^2+4^2} = \\sqrt{25} = 5$$\n",
    "\n",
    "\n",
    "These responses will mostly be manually graded. There is leeway for individual style in notation, so don't worry too much as long as you don't write anything that is false. \n",
    "\n",
    "### Code responses\n",
    "\n",
    "Whenever you see:\n",
    "\n",
    "```python\n",
    "answers = ...\n",
    "```\n",
    "\n",
    "You need to replace this section with some code that answers the questions and meets the specified criteria. Make sure you remove the 'raise' line when you do this (or your notebook will raise an error, regardless of any other code, and thus fail the grading tests).\n",
    "\n",
    "You should write the answer to the questions in those cells (the ones with `...`), but you can also add extra cells to explore / investigate things if you need / want to. \n",
    "\n",
    "Any cell with `grader.check(\"Question_id\")` statements in it is a test cell. You should not try to change or delete these cells. Note that there might be more than one assert that tests a particular question. Once you run the cell, you will see the public tests we provided for you.\n",
    "\n",
    "If a test does fail, reading the error that is printed out should let you know which test failed, which may be useful for fixing it.\n",
    "\n",
    "Note that some cells, including the test cells, may be read only, which means they won't let you edit them. If you cannot edit a cell - that is normal, and you shouldn't need to edit that cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1: Mixture of Bernoullis (1 Point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Q1.1 Mixture of Distributions\n",
    "Consider a mixture distribution of the form \n",
    "$$p(x) = \\sum_{k=1}^K \\pi_k p(x|k)$$\n",
    "where you may assume the elements of $x$ are discrete, and $\\vec \\pi$ is a $k$ dimensional vector that satisfy the following condition $\\sum_k \\pi_k = 1$. Denote the mean and covariance of $p(x|k)$ by $\\mu_k$ and $\\Sigma_k$, respectively.  Show that the mean of the mixture distribution is given by the following equation:\n",
    "$$\\mathbb{E}[\\text{x}] = \\sum_{k=1}^K \\pi_k\\mu_k$$\n",
    "\n",
    "To get there you may want to start with the definition of the mixture $p(x)$ above.  Reminder, the generic definition of expectation:\n",
    "$$\\mathbb{E}[\\text{x}] = \\sum_x x p(x)$$\n",
    "\n",
    "_Points:_ 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### Illustration of Bernoulli Mixture Model on MNIST dataset\n",
    "We illustrate the Bernoulli mixture model to model handwritten digits from the MNIST dataset. Here the digit images have been turned into binary vectors by setting all elements whose values exceed 0.5 to 1 and setting the remaining elements to 0. A sample of this binary-valued MNIST is shown below for just the digits 2, 3, and 4. In other words, the only two possible pixel intensities of the MNIST is now $0$ (fully black) and $1$ (fully white). In this setting, we can consider each individual pixel as a bernoulli random variable.\n",
    "\n",
    "![title](imgs/mnist_bernoulli.png)\n",
    "\n",
    "This approach would fit each digit with a Bernoulli parameter vector as long as the number of pixels in an image.  And we'd converge to the correct parameter vectors by running iterations of the EM algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Q1.2 Mixture of Bernoulli Distribution\n",
    "\n",
    "Consider the joint distribution of latent and observed variables for the Bernoulli distribution obtained by forming the product of $p(x|z,\\mu)$ (given by Bishop equation (9.52)):\n",
    "$$p(x|z,\\mu) = \\prod_{k=1}^K p(x|\\mu_k)^{z_k}$$\n",
    "and $p(z|\\pi)$ (given by Bishop equation (9.53)):\n",
    "$$p(z|\\pi) = \\prod_{k=1}^K \\pi_k^{z_k}$$\n",
    "Show that if we marginalize this joint distribution with respect to $z$, then we obtain the following equation:\n",
    "$$p(x|\\mu, \\pi) = \\sum_{k=1}^K \\pi_k p(x|\\mu_k)$$\n",
    "\n",
    "Hint: you might want to read the pages of Bishop noted above. Also when we say \"marginalize with respect to $z$\", you should review what that means and start with the definition of marginalizing over a latent variable. \n",
    "\n",
    "_Points:_ 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Q1.3 Mixture of Bernoulli Distribution M-step\n",
    "Show that if we maximize the expected complete-data log likelihood function (Bishop equation (9.55))\n",
    "$$\\mathbb{E}_z[\\ln p(\\text{Z,X}|\\mu, \\pi)] = \\sum_{n=1}^N\\sum_{k=1}^K\\gamma(z_{nk})\\biggl\\{\\ln \\pi_k + \\sum_{i=1}^D[x_{ni}\\ln\\mu_{ki} + (1-x_{ni})\\ln(1-\\mu_{ki})]  \\biggl\\}$$\n",
    "for a mixture of Bernoulli distributions with respect to $\\mu_k$, we obtain the M-step equation below:\n",
    "$$\\mu_k = \\frac{1}{N_k}\\sum_{n=1}^N\\gamma(z_{nk})x_n,$$\n",
    "that is, the k-th mean's MLE is a $\\gamma(z_{nk})$ weighted mean of the entire dataset.\n",
    "\n",
    "_Hint:_ you will want to start with the idea that we always go to when we want to maximize a convex function (as log likelihood is indeed convex). Make sure you are doing this process with respect to the variable you are trying to maximize.\n",
    "\n",
    "_Points:_ 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "# Q2: K-Means (3.5 Points)\n",
    "#### Algorithm Description\n",
    "\n",
    "Suppose we have a dataset $\\mathcal{D}=\\{x_1, \\dots, x_N\\}$ consisting of $N$ observations of a $D$-dimensional variable $x$. In clustering, the distortion measure of the cluster is given by \n",
    "\n",
    "$$\n",
    "J = \\sum_{n=1}^N\\sum_{k=1}^Kr_{nk} ||x_n-\\mu_k||^2\n",
    "$$\n",
    "\n",
    "where $\\mu_k$ is a prototype associated with the $k^{\\text{th}}$ cluster, and $r_{nk} \\in \\{0, 1\\}$ is a binary indicator variables where $k = 1, \\dots, K$ describing which of the $k$ cluster the data point $x_n$ is assigned to, so that if data point $x_n$ is assigned to cluster $k$ then $r_{nk} = 1$ and $r_{nj} = 0$ for $j \\neq k$.\n",
    "\n",
    "The objective of the K-Means algorithm is to find values for the $r_{nk}$ and the $\\mu_k$ so as to minimize the distortion measure, i.e.\n",
    "\n",
    "$$\n",
    "\\arg\\min_{r, \\mu} \\sum_{n=1}^N\\sum_{k=1}^Kr_{nk} ||x_n-\\mu_k||^2\n",
    "$$\n",
    "\n",
    "_Note:_ Compared with the objective function you saw in the lecture slides $\\arg\\min_C\\sum_{i=1}^k\\sum_{x_j\\in C_i} ||x_j - \\mu_{C_i}||^2$ this is a more nuanced way to express the K-means objective. In this question, we will explore the underlying update rules of the K-Means algorithms.\n",
    "\n",
    "#### Algorithm Pseudocode\n",
    "\n",
    "1. Choose some initial values for the $\\mu_k$ (Initialize the Centroids)\n",
    "2. Repeat until converged:\n",
    "    1. Keeping $\\mu_k$ fixed, minimizing $J$ w.r.t. $r_{nk}$ **(E-Step)**\n",
    "    2. Keeping $r_{nk}$ fixed, minimizing $J$ w.r.t. $\\mu_k$ **(M-Step)**\n",
    "\n",
    "#### Optimizing for $J$\n",
    "\n",
    "For the **(E-Step)**, when the $\\mu_k$ is fixed, it is rather simple to minimize the function value of $J$ since $J$ is a linear function of $r_{nk}$. The terms involving different $n$ are independent and so we can optimize for each $n$ separately by choosing $r_{nk}$ to be 1 for whichever value of $k$ gives the minimum value of $||x_n - \\mu_j||^2$. In other words, we simply assign the nth data point to the closest cluster center.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Question 2.1 Update Rule for $\\mu_k$\n",
    "\n",
    "Once we minimizing $J$ w.r.t. $r_{nk}$ while fixing $\\mu_k$, we will continue to minimize $J$ w.r.t. $\\mu_k$ fixing $r_{nk}$ fixed. Prove that\n",
    "$$\n",
    "\\hat \\mu_k = \\arg\\min_{\\mu_k}\\sum_{n=1}^N\\sum_{k=1}^Kr_{nk} ||x_n-\\mu_k||^2 = \\frac{\\sum_n r_{nk}x_n}{\\sum_n r_{nk}}\n",
    "$$\n",
    "\n",
    "*Hint:* Take the derivative of $J$ w.r.t. $\\mu_k$, and find the critical point.\n",
    "\n",
    "_Points:_ 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "The denominator in this expression is equal to the number of points assigned to cluster $k$, and so this result has a simple interpretation, namely set $\\mu k$ equal to the mean of all of the data points $x_n$ assigned to cluster $k$. For this reason, the procedure is known as the K-means algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rest of the question, you will implement your own version of the K-Means clustering algorithm. To begin with, let's create some toy data. This is borrowed from the lecture notebook.\n",
    "\n",
    "#### Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# things we will need to do stuff in this notebook\n",
    "import doctest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.datasets import make_blobs\n",
    "from scipy.stats import multivariate_normal\n",
    "from tqdm import tqdm\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "# two useful data viz libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# setup plotting in a notebook in a reasonable way\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "# default figure aesthetics I'll be using, \n",
    "# there are other choices, see seaborn docs\n",
    "sns.set_style(\"white\")\n",
    "sns.set_context(\"notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Dataset\n",
    "\n",
    "points, labels = make_blobs(\n",
    "    cluster_std=7.0, n_samples=1000, n_features=2, random_state=42, centers=[ [20,20], [-20,0], [20,-20]]\n",
    ")\n",
    "\n",
    "data_KMeans = pd.DataFrame(points,columns=['x','y'])\n",
    "\n",
    "# Define a helper function\n",
    "def plot_cluster(data:pd.DataFrame, km=None, plot_init=False, ax=None):\n",
    "    \"\"\"\n",
    "    helper function to plot out the clusters. When km is unspecified, it will\n",
    "    plot out the dataset. When km is specified, it will plot out the\n",
    "    results of the km's cluster.\n",
    "    \"\"\"\n",
    "    # Take a look at the cluster\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax = plt.axes()\n",
    "    if km is None:\n",
    "        sns.scatterplot(data=data,x='x',y='y',ax=ax);\n",
    "    else:\n",
    "        sns.scatterplot(data=data,x='x',y='y',ax=ax, hue=km.membership, palette='Paired');\n",
    "        plt.scatter(\n",
    "                km.centroids[:, 0],\n",
    "                km.centroids[:, 1],\n",
    "                s=100, marker=\",\", c=\"r\",\n",
    "                label=\"Centroids\"\n",
    "            )\n",
    "        plt.legend()\n",
    "    if plot_init:\n",
    "        plt.scatter(\n",
    "            km.initial_centroids[:, 0],\n",
    "            km.initial_centroids[:, 1],\n",
    "            s=100, facecolors='none', edgecolors='r',\n",
    "            label=\"Initial Centroid\"\n",
    "        )\n",
    "        plt.legend()\n",
    "    plt.axis('equal');\n",
    "    plt.axis('off');\n",
    "\n",
    "plot_cluster(data_KMeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 2.2 K-Means Implementation\n",
    "\n",
    "Implement each methods of the class `MyKMeans` below. We've provided doctests (public tests) for each of the methods. But be careful... just because you pass the doctests doesn't mean you're good.  These tests provide basic sanity checks, but do not extensively test the correctness of your implementation. If you want to check the validity of your code, you should write your own tests --  either high-level tests, such as examining the end results of the K-Means clusters, or low-level tests, such as asserting each data is in the correct shape and datatype.\n",
    "\n",
    "_Points:_ 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyKMeans():\n",
    "    \"\"\"\n",
    "    Your own KMeans implementation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, k) -> None:\n",
    "        # this hints you about the type of each arguments\n",
    "        self.k = k\n",
    "        self.data = np.array([[]])\n",
    "        self.initial_centroids = np.array([[]])\n",
    "        self.centroids = np.array([[]])\n",
    "        self.membership = np.array([])\n",
    "    \n",
    "    def euc_distances(self, target_pt:np.ndarray, pts:np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculating the euclidean distances between a single target point and \n",
    "        a numpy array of datapoints. Return a numpy array of distances.\n",
    "\n",
    "        Doctest/Example:\n",
    "        >>> km1 = MyKMeans(k=1)\n",
    "        >>> target_pt = np.array([0,0])\n",
    "        >>> pts = np.array([[0,0], [1,0], [0,1]])\n",
    "        >>> km1.euc_distances(target_pt, pts)\n",
    "        array([0., 1., 1.])\n",
    "        \"\"\"\n",
    "        # WRITE YOUR CODE HERE\n",
    "        ...\n",
    "\n",
    "    def _load_data(self, data) -> None:\n",
    "        \"\"\"\n",
    "        internal methods. assign data to self.data\n",
    "\n",
    "        Doctest/Example:\n",
    "        >>> km1 = MyKMeans(k=1)\n",
    "        >>> data = np.array([[0,0],[1,1]])\n",
    "        >>> km1._load_data(data)\n",
    "        >>> np.isclose(data, km1.data).all()\n",
    "        True\n",
    "        >>> \n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "\n",
    "    def _init_centroids(self):\n",
    "        \"\"\"\n",
    "        internal methods. randomly initialize k centroids by sampling k points without replacement\n",
    "        from the dataset. Assign the centroids to the self.centroids\n",
    "\n",
    "        Doctest/Example:\n",
    "        >>> km2 = MyKMeans(k=2)\n",
    "        >>> target_pt = np.array([0,0])\n",
    "        >>> data = np.array([[0,0], [0,0], [0,0]])\n",
    "        >>> km2._load_data(data)\n",
    "        >>> km2._init_centroids()\n",
    "        >>> km2.centroids.shape\n",
    "        (2, 2)\n",
    "        \"\"\"\n",
    "        # we provided this for you\n",
    "        idx = np.random.choice(self.data.shape[0], self.k, replace=False)\n",
    "        self.centroids = self.data[idx]\n",
    "        self.initial_centroids = self.data[idx]\n",
    "    \n",
    "    def E_step(self) -> None:\n",
    "        \"\"\"\n",
    "        Compute the E-step of the K-Means. This is calculating membership array \n",
    "        that indicates the membership of the data at that corresponding index. \n",
    "\n",
    "        For example, if self.membership is [0, 0, 1], this means that the first and \n",
    "        the second data points are closer to centroid 0 (self.centroid[0]), and the \n",
    "        third data point is closer to the center 1 (self.centroid[1])\n",
    "\n",
    "        Doctest/Example:\n",
    "        >>> km2 = MyKMeans(k=2)\n",
    "        >>> data = np.array([[0,0], [0,0], [0,0]])\n",
    "        >>> km2._load_data(data)\n",
    "        >>> km2._init_centroids()\n",
    "        >>> km2.E_step()\n",
    "        >>> km2.membership\n",
    "        array([0, 0, 0])\n",
    "        \"\"\"\n",
    "        # WRITE YOUR CODE HERE\n",
    "        ...\n",
    "\n",
    "    def M_step(self) -> None:\n",
    "        \"\"\"\n",
    "        Compute the M-step of the K-means. Update the self.centroids according\n",
    "        to the new membership array.\n",
    "\n",
    "        >>> km2 = MyKMeans(k=2)\n",
    "        >>> np.random.seed(42)\n",
    "        >>> data = np.array([[0], [0], [10], [10]])\n",
    "        >>> km2._load_data(data)\n",
    "        >>> km2._init_centroids()\n",
    "        >>> km2.E_step()\n",
    "        >>> km2.M_step()\n",
    "        >>> km2.centroids.shape\n",
    "        (2, 1)\n",
    "        >>> np.isclose(km2.centroids, np.array([[0], [10]])).all()\n",
    "        True\n",
    "        \"\"\"\n",
    "        # WRITE YOUR CODE HERE\n",
    "        ...\n",
    "\n",
    "\n",
    "    def fit(self, data, iteration=100) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Run the K-Means algorithm based on the dataset\n",
    "        \"\"\"\n",
    "        # we provided this for you\n",
    "        self._load_data(data)\n",
    "        self._init_centroids()\n",
    "        for _ in range(iteration):\n",
    "            self.E_step()\n",
    "            self.M_step()\n",
    "        return self.membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run the doctest\n",
    "doctest.testmod()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clusters Visualizations\n",
    "\n",
    "Run the following cell to take a look at the K-Means cluster. \n",
    "\n",
    "_(Not Graded, just FYI:)_ Run the following cell several times. What did you see? Are the clusters' shape consistent between each run? Are the color of the clusters consistent between each run?  Are the labels consistent between each run?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate MyKMeans Object\n",
    "km = MyKMeans(k=3)\n",
    "# Get out the membership array\n",
    "labels = km.fit(points)\n",
    "\n",
    "plot_cluster(data_KMeans, km)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Question 2.3 Effects of Centroid Initialization\n",
    "Take a look at the K-Mean cluster. \n",
    "\n",
    "![initialization](imgs/initialization.png)\n",
    "\n",
    "What is the differences between the clusters in the left and the clusters in the right? In what ways does the initialization of the centroids affect the clusters we got?\n",
    "\n",
    "\n",
    "_Points:_ 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### Question 2.4.1 Distortion Measures\n",
    "\n",
    "Given the `points` and `centroids` of the dataset, calculate the distortion measure given by this formula, and return that value. Remember, the distortion measure is given by:\n",
    "\n",
    "$$\n",
    "J = \\sum_{n=1}^N\\sum_{k=1}^Kr_{nk} ||x_n-\\mu_k||^2\n",
    "$$\n",
    "\n",
    "_Points:_ 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def distortion_measure(\n",
    "        points: np.ndarray, centroids: np.ndarray\n",
    "    ) -> float:\n",
    "    \"\"\"\n",
    "    Given the points and the centroids, calculate the \n",
    "    distortion measure this set of clusters.\n",
    "    \n",
    "    points: 2d numpy array containing the points\n",
    "    centroids: 2d numpy array containing the centroids\n",
    "    \"\"\"\n",
    "    # WRITE YOUR CODE HERE\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"241_distortion_measures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 2.4.2 Choose the Right K\n",
    "\n",
    "In real world scenarios, we as data scientists oftentimes do not have access to the knowledge of the true $k$ in the data generating process. Therefore, in this section, we explore how to choose the right value of the $k$ to achieve the desired clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Fit K-Means on different values of k\n",
    "costs = []\n",
    "for k in range(1, 9):\n",
    "    # instantiate KMeans Object\n",
    "    km = MyKMeans(k=k)\n",
    "    # Get out the membership array\n",
    "    labels = km.fit(points)\n",
    "    costs.append(distortion_measure(points, km.centroids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Visualize the Distortion Measure.\n",
    "plt.plot(range(1, 9), costs)\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"distortion measure\")\n",
    "plt.title(\"Distortion Measures of each $k$ values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "If your implementations is sound, you should have a plot looks like this.\n",
    "\n",
    "<img src=\"imgs/distortion.png\" alt=\"distortion\" width=\"600\"/>\n",
    "\n",
    "\n",
    "Which is the right K to choose? And why?\n",
    "\n",
    "_Points:_ 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Q3 K-Means with scikit-learn (1 Point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Q3.1 K-means on MNIST\n",
    "\n",
    "Let's explore doing a more realistic clustering task with K-means.  MNIST is a dataset of digits scanned from zip codes of snail mail letters. This particular version is reduced size... fewer samples (<1800 vs 70k), fewer grayscale values (16 vs 255) and downscaled (8x8 pixels vs 64x64 pixels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import rand_score, adjusted_rand_score\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "data_MNIST, labels = load_digits(return_X_y=True)\n",
    "(n_samples, n_features), n_digits = data_MNIST.shape, np.unique(labels).size\n",
    "\n",
    "print(f\"# digits: {n_digits}; # samples: {n_samples}; # features {n_features}\")\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "# heres a sample of 20 images to help you visualize just how ugly this level of downscaling/sampling is\n",
    "for i in range(20):\n",
    "    ax = fig.add_subplot(5, 5, i + 1, xticks=[], yticks=[])\n",
    "    # data is 28 x 28 pixels, grayscale\n",
    "    ax.imshow(data_MNIST[i].reshape(8,8), cmap=plt.cm.binary, interpolation='nearest')\n",
    "    # label the image with the true target value in the lower left corner\n",
    "    ax.text(0, 7, str(labels[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "In the cell below please do the following, in the following order:\n",
    "1. Create a KMeans instance with 10 clusters, plus the following arguments `init=\"k-means++\", n_init=4, random_state=777`\n",
    "1. Fit the KMeans with `data_MNIST`\n",
    "1. Create a pandas DataFrame called `kmm_pred` with the following two columns: \n",
    "    - 'true': which contains the true categories of each datapoint, the variable `labels` we already loaded \n",
    "    - 'pred': the predicted clustering of `data` from the fitted Kmeans\n",
    "    - NOTE: you MUST use exactly the column names in single quotes above\n",
    "1. Calculate the Rand score for the clustering, and store the value in a variable called `kmeans_rand`. NOTE: you need to pay attention to the order of arguments to this function, if you reverse them you may get a wrong answer.\n",
    "1. Calculate the Adjusted Rand score for the clustering, and store the value in a variable called `kmeans_adj_rand`. \n",
    "\n",
    "\n",
    "Use the scikit-learn docs to help you!\n",
    "\n",
    "\n",
    "\n",
    "_Points:_ 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kmeans = ...\n",
    "kmeans_pred = ...\n",
    "kmeans_pred['true'] = ...\n",
    "kmeans_rand = ...\n",
    "kmeans_adjusted_rand = ...\n",
    "\n",
    "print(kmeans_rand, kmeans_adjusted_rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"3_1_k_means_sklearn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Q3.2 Interpret the result\n",
    "The next two cells have visualizations that you can do to see what is going on with the predicted clusters and how they relate to the true labels and vice versa.  Take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "axs=kmeans_pred.hist(column='pred',by='true',sharey=True, figsize=(10,10), bins=range(11));\n",
    "for ax in axs.ravel():\n",
    "    ax.set_xlim((0,10))\n",
    "    ax.set_xticks(range(10))\n",
    "    old=ax.get_title()\n",
    "    ax.set_title('true='+old)\n",
    "    ax.set_xlabel('pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "axs=kmeans_pred.hist(column='true',by='pred',sharey=True, figsize=(10,10), bins=range(11));\n",
    "for ax in axs.ravel():\n",
    "    ax.set_xlim((0,10))\n",
    "    ax.set_xticks(range(10))\n",
    "    old=ax.get_title()\n",
    "    ax.set_title('pred='+old)\n",
    "    ax.set_xlabel('true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "OK, now in the next cell discuss your results above.  \n",
    "\n",
    "Please give us some insights... think deeply about the plots and empirical results in light of what you know about the clustering method, the metrics, and the visualizations.  If you just copy/paste some definitions and say \"yes/no\" you will not be getting full points.\n",
    "\n",
    "1. Describe what Rand score and Adjusted Rand score are. I suggest you look at the Wikipedia page as well as the scikit-learn docs.  Describe the difference between the metrics. Interpret the numbers you got for Rand vs Adjusted Rand in light of what the difference is in the metrics. \n",
    "1. Do you think that the clustering produced by KMeans is any good for predicting the true label?  Why or why not... please include evidence from the viz and the scores above.\n",
    "1. Regardless of what you said about the prediction above, do you think that the clustering can tell you anything about the data... like are there particular true labels that are MORE likely to be confused with others? What else might we learn from the clustering?\n",
    "\n",
    "_Points:_ 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "# Q4 Gaussian Mixture Model (GMM) (3.5 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def draw_ellipse(position, covariance, ax=None, **kwargs):\n",
    "    \"\"\"Draw an ellipse with a given position and covariance\"\"\"\n",
    "    ax = ax or plt.gca()\n",
    "    # Convert covariance to principal axes\n",
    "    if covariance.shape == (2, 2):\n",
    "        U, s, Vt = np.linalg.svd(covariance)\n",
    "        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\n",
    "        width, height = 2 * np.sqrt(s)\n",
    "    else:\n",
    "        angle = 0\n",
    "        width, height = 2 * np.sqrt(covariance)\n",
    "    \n",
    "    # Draw the Ellipse\n",
    "    for nsig in range(1, 4):\n",
    "        ax.add_patch(Ellipse(position, nsig * width, nsig * height,\n",
    "                             angle=angle, **kwargs))\n",
    "\n",
    "def plot_gmm(gmm, X, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    labels = gmm.fit(X).predict(X)\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='Set3')\n",
    "    ax.axis('equal')\n",
    "    w_factor = 0.2 / gmm.weights_.max()\n",
    "    \n",
    "    if gmm.covariance_type=='diag':\n",
    "        fullc = np.array([ np.diag(x) for x in gmm.covariances_])\n",
    "    elif gmm.covariance_type=='spherical':\n",
    "        fullc = np.array([ np.diag([x, x]) for x in gmm.covariances_])\n",
    "    elif gmm.covariance_type=='tied':\n",
    "        fullc = np.array( gmm.n_components*[gmm.covariances_])\n",
    "    elif gmm.covariance_type=='full':\n",
    "        fullc = gmm.covariances_\n",
    "    else:\n",
    "        raise NotImplemented\n",
    "        \n",
    "    for pos, covar, w in zip(gmm.means_, fullc, gmm.weights_):\n",
    "        alph = np.max([w*w_factor, 0.05])\n",
    "        draw_ellipse(pos, covar, ax=ax, alpha=alph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New Dataset\n",
    "In the following problems, you will implement you own version of Gaussian Mixture Model. But first, you will use the K-means you implemented in Q1 on a new dataset generated in the following cell. The new data are stored in the variable `X_gmm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformation = [[0.60834549, -0.63667341], [-0.40887718, 0.85253229]]\n",
    "X, y_gmm = make_blobs(\n",
    "    n_samples=[150, 100, 200], cluster_std=[1, 0.6, 1.5], centers=[[-1,-1],[6,4],[12,12]], random_state=42\n",
    ")\n",
    "X_gmm = np.dot(X, transformation)  # Anisotropic blobs\n",
    "data = pd.DataFrame(X_gmm,columns=['x','y'])\n",
    "plot_cluster(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Q4.1 Apply the K-means you implemented \n",
    "\n",
    "Use the K-means class you implemented in the Q1 to model the new dataset stored in `X_gmm`.\n",
    "\n",
    "_Points:_ 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(X_gmm,columns=['x','y'])\n",
    "\n",
    "# instantiate KMeans Object\n",
    "km = ...\n",
    "# Get out the membership array\n",
    "labels = ...\n",
    "\n",
    "plot_cluster(data, km)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"41_k_means_new_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Q4.2 Interpret the results of K-means from Q3.1\n",
    "According to the result of Q3.1, does K-means fit well to this new dataset? Why or why not? Explain.\n",
    "\n",
    "_Points:_ 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### Gaussian Mixture Model\n",
    "Gaussian Mixture Models (GMMs) are probabilistic models that assume data is generated from a mixture of $K$ Gaussian distributions, each with its own parameters: mean($\\mu_k$) and covariance($\\Sigma_k$). They are commonly used in clustering and density estimation, as they can flexibly capture the presence of multiple subpopulations within a dataset. \n",
    "$$p(\\text{x}) = \\sum_{k=1}^K \\pi_k \\mathcal{N}(\\text{x}|\\mu_k, \\Sigma_k)$$\n",
    "where $\\pi_k$(mixing coefficients) can be interpreted as proportion of data in clutser $k$ and \n",
    "$$\\sum_{k=1}^K \\pi_k = 1$$\n",
    "Another quantity that will play an important role is the *responsibility* conditoinal probability of $z_k$ given $x_n$, denoted as:\n",
    "$$\\gamma(z_{nk}) = \\frac{\\pi_k \\mathcal{N}(\\text{x}_n|\\mu_k, \\Sigma_k)}{\\sum_{j=1}^{\\text{K}}\\pi_j \\mathcal{N}(\\text{x}_n|\\mu_j, \\Sigma_j)}$$\n",
    "And $\\gamma{z_{nk}}$ can be viewed as the responsibility that cluster $k$ takes for explaning the obsevation $x_n$\n",
    "\n",
    "#### Algorithm Pseudocode\n",
    "Given a Gaussian Mixture Model, the goal is to maximize the log likelihood function with respect to the parameters(comprising the means($\\mu_k$), covariances($\\Sigma_k$) of the clusters, and the mixing coefficients($\\pi_k$))\n",
    "$$\\arg\\max_{\\pi, \\mu, \\Sigma} \\ln p(\\text{X}|\\pi, \\mu, \\Sigma) = \\sum_{n=1}^N\\ln\\biggl\\{\\sum_{k=1}^K \\pi_k \\mathcal{N}(\\text{x}|\\mu_k, \\Sigma_k)\\biggl\\}$$\n",
    "\n",
    "1. Initialize parameters $\\mu$, $\\Sigma$ and $\\pi$\n",
    "2. Repeat for some number of iterations or until converged:\n",
    "    1. Keep the parameters fixed, evaluate the responsibilities $\\gamma$. **(E-Step)**\n",
    "    2. Keep the responsibilities($\\gamma$) fixed, update the parameters $\\pi, \\mu, \\Sigma$ **(M-Step)**\n",
    "  \n",
    "You are provided with the skeleton codes for Gaussian Mixture Model and you will implement the E-step and M-step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMModel():\n",
    "    \n",
    "    def __init__(self, X, k, max_iters):\n",
    "        \"\"\"\n",
    "        This function initializes our parameters (mu, pi and sigma) and plots our data points.\n",
    "        \"\"\"\n",
    "        self.X = X # Data\n",
    "        self.k = k # Number of cluster/latents\n",
    "        self.max_iters = max_iters\n",
    "        self.dim = self.X.shape[1] # Equals 2, as we are considering 2D points\n",
    "        self.N = self.X.shape[0] # Equals the number of points in the dataset\n",
    "        \"\"\"\n",
    "        Here we initialize mu, pi, sigma and gamma.\n",
    "        \"\"\"\n",
    "        self.mu = np.copy(X[np.random.choice(self.N, self.k, False), :]) # Shape: k x dim\n",
    "        self.pi =  np.ones(self.k)/self.k # Shape: k\n",
    "        self.sigma = np.array([5.0 * np.identity(self.dim) for i in range(self.k)]) # Shape: k x dim x dim\n",
    "        self.gamma = np.zeros((self.N, self.k)) # Shape: N x k (used in E-step and M-step)\n",
    "        \"\"\"\n",
    "        The following part is used for plotting\n",
    "        \"\"\"\n",
    "        x,y = np.meshgrid(np.sort(self.X[:,0]),np.sort(self.X[:,1]))\n",
    "        self.XY = np.array([x.flatten(),y.flatten()]).T\n",
    "\n",
    "    def plot_gmm_data(self, title, colors=None):\n",
    "        \"\"\"\n",
    "        This function creates a scatter plot of all the data points. It also creates a contour plot of the probability \n",
    "        distributions of each of the clusters (specified by mu, pi and sigma)\n",
    "        \"\"\"\n",
    "        fig = plt.figure(figsize=(6,6))\n",
    "        ax0 = fig.add_subplot(111)\n",
    "        # ax0.scatter(self.X[:,0], self.X[:,1], c=colors)\n",
    "        data = pd.DataFrame(self.X,columns=['x','y'])\n",
    "        sns.scatterplot(data=data,x='x',y='y',ax=ax0, hue=colors)\n",
    "        ax0.set_title(title)\n",
    "        ax0.axis('equal')\n",
    "        ax0.axis(xmin=-4,xmax=8)\n",
    "        ax0.axis(ymin=-4,ymax=8)\n",
    "        for m,c in zip(self.mu,self.sigma):\n",
    "            # c += self.sigma_correction\n",
    "            multi_normal = multivariate_normal(mean=m,cov=c)\n",
    "            # ax0.contour(np.sort(self.X[:,0]),np.sort(self.X[:,1]),multi_normal.pdf(self.XY).reshape(len(self.X),len(self.X)),colors='black',alpha=0.3)\n",
    "            draw_ellipse(m, c, ax=None, alpha=0.08)\n",
    "            ax0.scatter(m[0],m[1],c='r',zorder=10,s=30, marker=\",\")\n",
    "        # ax0.legend()\n",
    "        # return fig\n",
    "\n",
    "    def fit(self, graph=True, disable=False):\n",
    "        if graph:\n",
    "            self.plot_gmm_data('Initial State')\n",
    "        for i in tqdm(range(self.max_iters), disable=disable):\n",
    "            self.expectation()\n",
    "            self.maximization()\n",
    "        if graph:\n",
    "            color_labels = np.argmax(self.gamma, axis=1)\n",
    "            self.plot_gmm_data('Final State', color_labels)\n",
    "            plt.legend()\n",
    "\n",
    "        return self.mu, self.sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Q4.3 E-step of GMM?\n",
    "\n",
    "What is the formula of the E-step of GMM? \n",
    "\n",
    "$\\gamma(z_{nk}) = \\dots$\n",
    "\n",
    "_Points:_ 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Q4.4 Implement the E-step of GMM\n",
    "Complete the `expectation` function of GMM based on the formula from Q3.3.\n",
    "\n",
    "_Points:_ 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def expectation(self):\n",
    "    \"\"\"\n",
    "    Perform the E-Step of GMM. Update the responsibility based on the parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    return self.gamma\n",
    "\n",
    "# assign this method to GMModel module    \n",
    "GMModel.expectation = expectation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"44_gmm_e_step_implementation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Q4.5 What is the formula of M-step of GMM?\n",
    "Write down the formula for the M-step of GMM that updates the parameters of GMM. \n",
    "\n",
    "_Points:_ 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Q4.6 Implement M-step of GMM\n",
    "Complete the `maximization` function based on the formula from Q3.5.\n",
    "\n",
    "_Points:_ 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def maximization(self):\n",
    "\n",
    "    ...\n",
    "    \n",
    "    # Update the sigma parameter\n",
    "    self.sigma = np.zeros((self.k, self.dim, self.dim))\n",
    "    for j in range(self.k):\n",
    "        for i in range(self.N):\n",
    "            c = self.X[i] - self.mu[j]\n",
    "            self.sigma[j] += self.gamma[i,j] * np.outer(c,c)\n",
    "        self.sigma[j] /= N_k[j]\n",
    "\n",
    "GMModel.maximization = maximization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"46_gmm_m_step_implementation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Q4.7 Interpret the result\n",
    "Comment on the result of GMM. How is it different from that of K-means you get from Q3.1\n",
    "\n",
    "_Points:_ 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "k = 3\n",
    "max_iterations = 1000\n",
    "em = GMModel(X_gmm, k, max_iterations)\n",
    "em.mu = np.array([X_gmm[10], X_gmm[100], X_gmm[300]]) # set a fixed starting point\n",
    "mu, sigma = em.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5: Gaussian Mixture Model with Scikit-Learn (1 Point)\n",
    "\n",
    "Let's try to use the same MNIST dataset from before but use a Gaussian Mixture Model.\n",
    "\n",
    "You are responsible for importing and using the correct class from scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Q5.1 GMM on MNIST\n",
    "In the cell below please do the following, in the following order:\n",
    "- Create a Mixture of Gaussians with 10 components plus the following arguments: `random_state=777, covariance_type='tied'`\n",
    "1. Fit the Mixture Model with `data`\n",
    "1. Create a pandas DataFrame called `gmm_pred` with the following two columns: \n",
    "    - 'true': which contains the true categories of each datapoint, the variable `labels` we already loaded \n",
    "    - 'pred': the predicted clustering of `data` from the fitted mixture model\n",
    "    - NOTE: you MUST use exactly the column names in single quotes above\n",
    "1. Calculate the Rand score for the clustering, and store the value in a variable called `gmm_rand`. NOTE: you need to pay attention to the order of arguments to this function, if you reverse them you may get a wrong answer.\n",
    "1. Calculate the Adjusted Rand score for the clustering, and store the value in a variable called `gmm_adj_rand`. NOTE: you need to pay attention to the order of arguments to this function, if you reverse them you may get a wrong answer.\n",
    "\n",
    "\n",
    "_Points:_ 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "data, labels = load_digits(return_X_y=True)\n",
    "(n_samples, n_features), n_digits = data.shape, np.unique(labels).size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "gmm = ...\n",
    "gmm_pred = ...\n",
    "gmm_pred['true'] = ...\n",
    "gmm_rand = ...\n",
    "gmm_adjusted_rand = ...\n",
    "print(gmm_rand, gmm_adjusted_rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"51_gmm_mnist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Q5.2 Interpret the GMM results\n",
    "The next two cells have visualizations that you can do to see what is going on with the predicted clusters and how they relate to the true labels and vice versa.  Take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "axs=gmm_pred.hist(column='pred',by='true',sharey=True, figsize=(10,10), bins=range(11));\n",
    "for ax in axs.ravel():\n",
    "    ax.set_xlim((0,10))\n",
    "    ax.set_xticks(range(10))\n",
    "    old=ax.get_title()\n",
    "    ax.set_title('true='+old)\n",
    "    ax.set_xlabel('pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "axs=gmm_pred.hist(column='true',by='pred',sharey=True, figsize=(10,10), bins=range(11));\n",
    "for ax in axs.ravel():\n",
    "    ax.set_xlim((0,10))\n",
    "    ax.set_xticks(range(10))\n",
    "    old=ax.get_title()\n",
    "    ax.set_title('pred='+old)\n",
    "    ax.set_xlabel('true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "OK, now in the next cell discuss your results above.  \n",
    "\n",
    "Please give us some insights... think deeply about the plots and empirical results in light of what you know about the clustering method, the metrics, and the visualizations.  If you just copy/paste some definitions and say \"yes/no\" you will not be getting full points.\n",
    "\n",
    "1. Do you think that the clustering produced by the Mixture Model is any good for predicting the true label?  Why or why not... please include evidence from the viz and the scores above. Compare the clustering produced by the Mixture Model with the one produced by the KMeans. \n",
    "\n",
    "1. Given the covariance argument passed to the Mixture Model, and the fact that the Mixture Model produced better results, what does that imply about the strucuture of the problem?\n",
    "\n",
    "\n",
    "\n",
    "_Points:_ 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## The End of A2\n",
    "\n",
    "Have a look back over your answers, and also make sure to `Restart & Run All` from the kernel menu to double check that everything is working properly. This restarts everything and runs your code from top to bottom.\n",
    "\n",
    "Once you're happy with your work, click the disk icon to save, and submit the zip file onto gradescope. **You MUST submit all the required component to receive credit.**\n",
    "\n",
    "Note that you can submit at any time, but **we grade your most recent submission**. This means that **if you submit an updated notebook after the submission deadline, it will be marked as late**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note:_ If you encountered `LatexFailed` message during exporting, or has the `AttributeError: module 'nbconvert' has no attribute 'pdf'` error, it indicates that your $\\LaTeX$ code is not correct. Try to the LaTeX syntax error by scrolling up to see the LaTeX error message. If you need any additional helps, please make a private post on campuswire and we are happy to help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit.\n",
    "\n",
    "Please make sure to see the output of the gradescope autograder. You are responsible for waiting and ensuring that the autograder is executing normally for your submission. Please create a campuswire post if you see errors in autograder execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.export(force_save=True, run_tests=True, files=['imgs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "22_KMeans_implementation": {
     "name": "22_KMeans_implementation",
     "points": 2.1,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "241_distortion_measures": {
     "name": "241_distortion_measures",
     "points": 0.4,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert callable(distortion_measure)\n>>> (pts, centroids) = (np.array([[0], [0], [0]]), np.array([[0]]))\n>>> assert np.isclose(0, distortion_measure(pts, centroids))\n",
         "hidden": false,
         "locked": false,
         "points": 0
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "3_1_k_means_sklearn": {
     "name": "3_1_k_means_sklearn",
     "points": 0.4,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert kmeans_pred.shape == (1797, 2)\n>>> assert 'pred' in kmeans_pred.columns\n>>> assert 'true' in kmeans_pred.columns\n",
         "hidden": false,
         "locked": false,
         "points": 0
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "41_k_means_new_data": {
     "name": "41_k_means_new_data",
     "points": 0.3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isinstance(km, MyKMeans)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "44_gmm_e_step_implementation": {
     "name": "44_gmm_e_step_implementation",
     "points": 0.6,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> k = 1\n>>> X_simple = np.array([[1, 1], [0, 0], [0, 1], [1, 0], [5, 5], [4, 5], [5, 4], [4, 4]])\n>>> em = GMModel(X_simple, k, max_iters=1)\n>>> em.mu = np.array([[1, 1]])\n>>> assert callable(em.expectation)\n>>> _ = em.expectation()\n>>> expected_gamma = np.ones((8, 1))\n>>> assert em.gamma.shape[0] == len(X_simple)\n>>> assert em.gamma.shape[1] == k\n>>> assert np.allclose(em.gamma, expected_gamma)\n",
         "hidden": false,
         "locked": false,
         "points": 0
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "46_gmm_m_step_implementation": {
     "name": "46_gmm_m_step_implementation",
     "points": 0.6,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> k = 1\n>>> X_simple = np.array([[1, 1], [0, 0], [0, 1], [1, 0], [5, 5], [4, 5], [5, 4], [4, 4]])\n>>> em = GMModel(X_simple, k, max_iters=100)\n>>> em.mu = np.array([[1, 1]])\n>>> assert callable(em.expectation)\n>>> assert callable(em.maximization)\n>>> _ = em.expectation()\n>>> em.maximization()\n>>> expected_mu = np.array([[2.5, 2.5]])\n>>> expected_sigma = np.array([[[4.25, 4.0], [4.0, 4.25]]])\n>>> assert np.allclose(em.mu, expected_mu)\n>>> assert np.allclose(em.sigma, expected_sigma)\n",
         "hidden": false,
         "locked": false,
         "points": 0
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "51_gmm_mnist": {
     "name": "51_gmm_mnist",
     "points": 0.4,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert gmm_pred.shape == (1797, 2)\n>>> assert 'pred' in gmm_pred.columns\n>>> assert 'true' in gmm_pred.columns\n",
         "hidden": false,
         "locked": false,
         "points": 0
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
